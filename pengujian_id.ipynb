{"cells":[{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6129,"status":"ok","timestamp":1692790078233,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"MVjyr2WnBHKa","outputId":"f9e25ebe-4aa8-483a-a493-f5b2bd793724"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://download.pytorch.org/whl/cu117\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11591,"status":"ok","timestamp":1692790089815,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"YX0cE-7rPpup","outputId":"315899bf-2bdd-4e4d-ccf2-f60feef971e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: sent2vec in /usr/local/lib/python3.10/dist-packages (0.3.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.16.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from sent2vec) (2.0.1+cu118)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from sent2vec) (4.3.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from sent2vec) (7.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[sentencepiece]) (4.7.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (1.2.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (1.1.3)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->sent2vec) (2.0.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (1.12)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->sent2vec) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->sent2vec) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->sent2vec) (16.0.6)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->sent2vec) (1.3.0)\n"]}],"source":["!pip install transformers[sentencepiece] datasets spacy scipy networkx numpy sent2vec pyngrok pandas nltk"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10920,"status":"ok","timestamp":1692790100729,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"hnYwLtaOxTEw","outputId":"86921453-d994-48ca-a093-075f8210fe5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n"]}],"source":["!pip install rouge_score evaluate"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1692790100730,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"Hb73KE3bPtVk","outputId":"0928cff2-a654-4f99-95cc-95fbfdf2f909"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# import library yang dibutuhkan\n","import networkx as nx\n","import numpy as np\n","import torch\n","from transformers import BertTokenizer, BertModel\n","import numpy as np\n","\n","from scipy import spatial\n","from sent2vec.vectorizer import Vectorizer\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.probability import FreqDist\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","from transformers import TFPegasusForConditionalGeneration, PegasusTokenizerFast\n","\n","import re\n","import unicodedata\n","\n","import pandas as pd\n","from rouge_score import rouge_scorer\n","from openpyxl import Workbook\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"fNSLBwSEQLQ4"},"source":["**PERINGKASAN BAHASA INDONESIA**"]},{"cell_type":"code","source":["# Load the tokenizer and model\n","tokenizer_bert_id = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model_bert_id = BertModel.from_pretrained('bert-base-multilingual-cased')"],"metadata":{"id":"E9_BFYk719qq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790126213,"user_tz":-420,"elapsed":25491,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"21e65289-b1c5-4813-a040-0082e53b058f"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 95abadbe-073a-423a-83bb-61b10837f299)')' thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt\n","WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 95abadbe-073a-423a-83bb-61b10837f299)')' thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt\n","'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ae6ff672-1019-4095-ac49-976db861215f)')' thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\n","WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ae6ff672-1019-4095-ac49-976db861215f)')' thrown while requesting HEAD https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json\n"]}]},{"cell_type":"code","source":["def vector_id(list_sentences):\n","\n","  # Initialize an empty array to store sentence embeddings\n","  sentence_embeddings = []\n","\n","  # Iterate through sentences and get embeddings\n","  for text in list_sentences:\n","      input_ids = tokenizer_bert_id.encode(text, add_special_tokens=True, padding=True, truncation=True)\n","      input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","      with torch.no_grad():\n","          outputs = model_bert_id(input_ids)\n","          cls_embedding = outputs.last_hidden_state[:, 0, :]\n","          embedding = cls_embedding[0]\n","\n","      sentence_embeddings.append(embedding.numpy())\n","\n","  return sentence_embeddings"],"metadata":{"id":"5qDn-MdL2AqG","executionInfo":{"status":"ok","timestamp":1692790126215,"user_tz":-420,"elapsed":14,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","execution_count":45,"metadata":{"id":"hM-FryE9P-Q6","executionInfo":{"status":"ok","timestamp":1692790126216,"user_tz":-420,"elapsed":14,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["#1. PRE-PROCESSING\n","def preprocess_id(text):\n","\n","  # Segmentasi (pisahkan text per kalimat dan masukkan kedalam list)\n","  sentences = sent_tokenize(text)\n","\n","  # Membersihkan setiap kalimat\n","  filtered_sentences = []\n","  for i in range(len(sentences)):\n","\n","    text = sentences[i]\n","\n","    # Tokenisasi (memecah kalimat menjadi kata)\n","    words = word_tokenize(text.lower())\n","\n","    # Membersihkan tanda baca\n","    words = [word for word in words if word.isalnum()]\n","\n","    # Menghapus stopwords\n","    stop_words = set(stopwords.words(\"indonesian\"))\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Melakukan lemmatisasi\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    # Menggabungkan kembali kata yang telah dibersihkan menjadi kalimat\n","    result = \" \".join(lemmatized_words)\n","\n","    filtered_sentences.append(result)\n","  return filtered_sentences"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"81sWn6YUUH5c","executionInfo":{"status":"ok","timestamp":1692790126217,"user_tz":-420,"elapsed":13,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# 2. PERINGKASAN EKSTRAKTIF\n","\n","def extractive_sum_id(filtered_sentences, sentences, n):\n","  # mengubah kalimat menjadi vektor\n","  vectors = vector_id(filtered_sentences)\n","\n","  # menghitung similarity matrix (matriks kemiripan antar kalimat)\n","  similarity_matrix = []\n","  for i in range(len(vectors)):\n","    row = []\n","    for j in range(len(vectors)):\n","      row.append(spatial.distance.cosine(vectors[i], vectors[j]))\n","    similarity_matrix.append(row)\n","\n","  # konversi matrix menjadi graph\n","  graph = nx.from_numpy_array(np.array(similarity_matrix))\n","\n","  # melakukan perangkingan\n","  scores = nx.pagerank(graph)\n","\n","  # mengambil top rank (kalimat dengan score tertinggi)\n","  sentences_size = len(sentences)\n","  num_sentences = round((sentences_size + 1) / n)\n","\n","  top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n","  summary = [sentences[i] for i in top_sentence_indices]\n","\n","  # hasil peringkasan ekstraktif\n","\n","  return \" \".join(summary)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"_16U6olJUMnB","executionInfo":{"status":"ok","timestamp":1692790126217,"user_tz":-420,"elapsed":12,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# 3. PERINGKASAN ABSTRAKTIF\n","\n","# membersihkan teks\n","def text_cleaning(input_string):\n","    lowercase = input_string.lower()\n","    remove_link = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)', '', lowercase).replace(\"&amp;\",\"&\")\n","    remove_bullet = \"\\n\".join([T for T in remove_link.split('\\n') if '•' not in T and \"baca juga:\" not in T])\n","    remove_accented = unicodedata.normalize('NFKD', remove_bullet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    remove_parentheses = re.sub(\"([\\(\\|]).*?([\\)\\|])\", \"\\g<1>\\g<2>\", remove_accented)\n","    remove_punc = re.sub(r\"[^\\w\\d.\\s]+\",' ', remove_parentheses)\n","    remove_num_dot = re.sub(r\"(?<=\\d)\\.|\\.(?=\\d)|(?<=#)\\.\",\"\", remove_punc)\n","    remove_extra_whitespace =  re.sub(r'^\\s*|\\s\\s*', ' ', remove_num_dot).strip()\n","    return \".\".join([s for s in remove_extra_whitespace.strip().split('.') if len(s.strip())>10]).replace(\"_\",\"\")"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49468,"status":"ok","timestamp":1692790175674,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"Pa38ACphUtOv","outputId":"793aa7aa-9eda-4d70-effc-9f1df2d9fd23"},"outputs":[{"output_type":"stream","name":"stderr","text":["'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 26abe93e-51c3-4788-81d7-d6b0b5fbd0ca)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/config.json\n","WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 26abe93e-51c3-4788-81d7-d6b0b5fbd0ca)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/config.json\n","All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n","\n","All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at thonyyy/pegasus_indonesian_base-finetune.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n","'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 28e3f395-80ca-411c-9db4-b2f79e482111)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/generation_config.json\n","WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 28e3f395-80ca-411c-9db4-b2f79e482111)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/generation_config.json\n","'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2d1308f9-a9ff-4a8f-8307-11b6f98740c3)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/tokenizer_config.json\n","WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2d1308f9-a9ff-4a8f-8307-11b6f98740c3)')' thrown while requesting HEAD https://huggingface.co/thonyyy/pegasus_indonesian_base-finetune/resolve/main/tokenizer_config.json\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["model_name2 = \"thonyyy/pegasus_indonesian_base-finetune\"\n","model2 = TFPegasusForConditionalGeneration.from_pretrained(model_name2)\n","tokenizer2 = PegasusTokenizerFast.from_pretrained(model_name2)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"1q6pf1qNVDXM","executionInfo":{"status":"ok","timestamp":1692790175675,"user_tz":-420,"elapsed":10,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["def abstractive_sum_id(text):\n","  clean_text = text_cleaning(text)\n","  inputs = tokenizer2(clean_text, return_tensors = 'tf')\n","  summary_ids = model2.generate(**inputs, max_new_tokens = 256, length_penalty=2.0, num_beams=4, early_stopping=True)\n","  summary = tokenizer2.batch_decode(summary_ids, skip_special_tokens=True)\n","\n","  return summary[0]"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"TbAivhhaVlh3","executionInfo":{"status":"ok","timestamp":1692790175676,"user_tz":-420,"elapsed":10,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# MERINGKAS TEKS BAHASA\n","def summy_id(text, n):\n","\n","  # memasukkan kalimat kedalam list (untuk memudahkan print output)\n","  sentences = sent_tokenize(text)\n","\n","  # 1. PRE-PROCESSING\n","  result = preprocess_id(text)\n","\n","  # 2. PERINGKASAN EKSTRAKTIF\n","  summary1 = extractive_sum_id(result, sentences, n)\n","\n","  # 3. PERINGKASAN ABSTRAKTIF\n","  summary = abstractive_sum_id(summary1)\n","  summary = summary.replace(\"<pad>\", \"\")\n","  summary = summary.replace(\"</s>\", \"\")\n","  summary = summary.replace(\".<n>\", \".\\n\")\n","\n","  return summary"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"XSiDu1RKV6Vq","executionInfo":{"status":"ok","timestamp":1692790175677,"user_tz":-420,"elapsed":10,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["text = \"\"\"\n","\n","Sampah selalu kita temukan mengotori lingkungan di sekitar kita. Maka wajar karena hal itu seringkali sampah menjadi masalah lingkungan yang serius harus ditangani.\n","\n","Sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. Walaupun sampah jelas-jelas membuat lingkungan tidak nyaman tetapi anehnya kesadaran kita terhadap lingkungan masih jauh dari cukup.\n","\n","Masih banyak di antara kita yang tidak memperhatikan membuang sampah pada tempatnya. Mereka baru menyadari pentingnya membuang sampah secara disiplin, ketika mulai banyak rusaknya lingkungan diakibatkan oleh sampah yang menumpuk.\n","\n","Pada akhirnya kondisi ini telah membuat banyak orang menjadi sadar bahwa mengelola sampah dengan bijak sangatlah penting untuk menjamin rasa nyaman lingkungan juga memperhatikan kesehatan.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":14551,"status":"ok","timestamp":1692790190219,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"AhZYYMmsWCcD"},"outputs":[],"source":["result_id = summy_id(text, 2)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1692790190219,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"FMmdy3e0WLL3","outputId":"d31845bd-954e-4b04-c977-5ff1eca8098a"},"outputs":[{"output_type":"stream","name":"stdout","text":["sampah bisa membuat suasana menjadi rusak seketika karena bau menyengat. sampah bisa membuat suasana menjadi rusak seketika karena bau menyengat\n"]}],"source":["print(result_id)"]},{"cell_type":"markdown","metadata":{"id":"r1iV6cfixr-4"},"source":["**PERINGKASAN BAHASA INDONESIA PENGUJIAN**"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"_rCXC7whxxWT","executionInfo":{"status":"ok","timestamp":1692790190220,"user_tz":-420,"elapsed":9,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# 1. TEXTRANK\n","def f_textrank_id(text, n):\n","  sentences = sent_tokenize(text)\n","\n","  result = preprocess_id(text)\n","\n","  # melakukan peringkasan ekstraktif\n","  summary = extractive_sum_id(result, sentences, n)\n","\n","  return summary"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"6GxeboCXyDUy","executionInfo":{"status":"ok","timestamp":1692790190220,"user_tz":-420,"elapsed":8,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# 2. PEGASUS\n","def f_pegasus_id(text):\n","  summary = abstractive_sum_id(text)\n","  return summary"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"RA1ZFXvJyGY9","executionInfo":{"status":"ok","timestamp":1692790190221,"user_tz":-420,"elapsed":9,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["# 3. HYBRID\n","def f_hybrid_id(text, n):\n","  summary = summy_id(text, n)\n","  return summary"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"ON7r8DE33-5F","executionInfo":{"status":"ok","timestamp":1692790190221,"user_tz":-420,"elapsed":9,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["text2 = \"\"\"\n","\n","Sampah selalu kita temukan mengotori lingkungan di sekitar kita. Maka wajar karena hal itu seringkali sampah menjadi masalah lingkungan yang serius harus ditangani.\n","\n","Sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. Walaupun sampah jelas-jelas membuat lingkungan tidak nyaman tetapi anehnya kesadaran kita terhadap lingkungan masih jauh dari cukup.\n","\n","Masih banyak di antara kita yang tidak memperhatikan membuang sampah pada tempatnya. Mereka baru menyadari pentingnya membuang sampah secara disiplin, ketika mulai banyak rusaknya lingkungan diakibatkan oleh sampah yang menumpuk.\n","\n","Pada akhirnya kondisi ini telah membuat banyak orang menjadi sadar bahwa mengelola sampah dengan bijak sangatlah penting untuk menjamin rasa nyaman lingkungan juga memperhatikan kesehatan.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":58,"metadata":{"id":"LeBYn8F9438l","executionInfo":{"status":"ok","timestamp":1692790224077,"user_tz":-420,"elapsed":33864,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"outputs":[],"source":["hasil_textrank_id = f_textrank_id(text2, 3)\n","hasil_pegasus_id = f_pegasus_id(text2)\n","hasil_hybrid_id = f_hybrid_id(text2, 2)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1692790224079,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"3FsYP7iq48cJ","outputId":"e52a16ce-8b41-4222-925c-1561ac785131"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. Pada akhirnya kondisi ini telah membuat banyak orang menjadi sadar bahwa mengelola sampah dengan bijak sangatlah penting untuk menjamin rasa nyaman lingkungan juga memperhatikan kesehatan. Maka wajar karena hal itu seringkali sampah menjadi masalah lingkungan yang serius harus ditangani.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":59}],"source":["hasil_textrank_id"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1692790224691,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"5Q76fe4S4_En","outputId":"145fee2c-d7da-4462-8f24-54d2ef0df259"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. padahal sampah jelas jelas membuat lingkungan tidak nyaman tetapi anehnya membuat lingkungan masih jauh dari cukup banyak'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}],"source":["hasil_pegasus_id"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1692790224692,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"N5SEbyXJ5Bd_","outputId":"126d0c35-e035-4a24-9f31-57f0e7bafa0f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'sampah bisa membuat suasana menjadi rusak seketika karena bau menyengat. sampah bisa membuat suasana menjadi rusak seketika karena bau menyengat'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}],"source":["hasil_hybrid_id"]},{"cell_type":"markdown","metadata":{"id":"GLYWES7H5bxD"},"source":["**PENGUJIAN PADA MASING MASING DATASET**"]},{"cell_type":"code","source":["# import library yang dibutuhkan\n","import os\n","import json\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import datasets\n","import pyarrow as pa\n","import pyarrow.dataset as ds\n","import pandas as pd\n","from datasets import Dataset\n","from tqdm import tqdm\n","from transformers import pipeline\n","from datasets import load_dataset, load_metric\n"],"metadata":{"id":"bIMFa1-Aouc0","executionInfo":{"status":"ok","timestamp":1692790224693,"user_tz":-420,"elapsed":17,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Unie_o2eopQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790227629,"user_tz":-420,"elapsed":2953,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"a3b6e71a-82ee-4cae-d698-b54254ec8b73"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# global config\n","\n","sns.set_style('ticks')"],"metadata":{"id":"I9PUQDceo0SL","executionInfo":{"status":"ok","timestamp":1692790227630,"user_tz":-420,"elapsed":13,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Mendeteksi file didalam diretori indosum\n","DATASET_ROOT = '/content/drive/MyDrive/NLP/fine_tune2/indosum'\n","\n","files_id_dir = os.listdir(DATASET_ROOT)\n","test_files = []\n","\n","for filename in files_id_dir:\n","    if 'test.01' in filename:\n","        test_files.append(filename\n","        )\n","\n","test_files"],"metadata":{"id":"egU8XHQBo4ZX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790227631,"user_tz":-420,"elapsed":13,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"30d20904-1436-4251-d320-8ebd4d04bbc1"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['test.01.jsonl']"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["# fungsi mengubah file menjadi json objek\n","def load_file_to_json_list(filename):\n","    file = os.path.join(DATASET_ROOT, filename)\n","    data = []\n","    with open(file, 'r') as f:\n","        json_list = list(f)\n","        for json_str in tqdm(json_list, desc=f'Loading data {filename}'):\n","            d = json.loads(json_str)\n","            data.append(d)\n","    return data\n","\n","\n","# mengubah label menjadi string dictionary\n","def label_to_dict_str(label_list):\n","    label_dict = {} # key = paragraph_id : value = label list\n","    for i, label in enumerate(label_list[:]):\n","        label_dict[i] = label\n","\n","    json_str = json.dumps(label_dict)\n","    num = len(label_dict)\n","    return json_str, num\n","\n","# mengubah paragraf menjadi list\n","def paragraph_to_dict_str(paragraph_list):\n","    paragraph_dict = {} # key = paragraph_id : value = paragraph list\n","    for i, paragraph in enumerate(paragraph_list):\n","        new_paragraph = []\n","        for sentence in paragraph:\n","            sentence = ' '.join(sentence)\n","            new_paragraph.append(sentence)\n","        paragraph_dict[i] = new_paragraph\n","\n","    json_str = json.dumps(paragraph_dict)\n","    num = len(paragraph_dict)\n","    return json_str, num\n","\n","# mengubah paragraf menjadi text\n","def paragraph_to_text(raw_paragraph_list):\n","    new_paragraph_list = []\n","    for i, paragraph in enumerate(raw_paragraph_list):\n","        paragraph_list = []\n","        for sentence in paragraph:\n","            sentence = ' '.join(sentence)\n","            paragraph_list.append(sentence)\n","\n","        new_paragraph = ' '.join(paragraph_list)\n","        new_paragraph_list.append(new_paragraph)\n","\n","    paragraph_str = ' '.join(new_paragraph_list)\n","    return paragraph_str\n","\n","# mengubah summary menjadi string dictionary\n","def summary_to_dict_str(summary_list):\n","    summary_dict = {} # key = summary_id : value = summary sentence\n","    for i, summary in enumerate(summary_list):\n","        summary_dict[i] = ' '.join(summary)\n","\n","    json_str = json.dumps(summary_dict)\n","    num = len(summary_dict)\n","    return json_str, num\n","\n","def summary_to_text(raw_summary_list):\n","    summary_list = []\n","    for i, summary in enumerate(raw_summary_list):\n","        summary_list.append(' '.join(summary))\n","\n","    summary_str = ' '.join(summary_list)\n","    return summary_str\n","\n","\n","# melakukan alter data json\n","def alter_json_data(json_list_data, filename=''):\n","    new_json_list = []\n","    for json_data in tqdm(json_list_data, desc=f'Altering json data {filename}'):\n","        json_data = json_data.copy()\n","        json_data['gold_labels'], _ = label_to_dict_str(json_data['gold_labels'])\n","        json_data['news_text'] = paragraph_to_text(json_data['paragraphs'])\n","        json_data['paragraphs'], num_paragraph = paragraph_to_dict_str(json_data['paragraphs'])\n","        json_data['num_of_paragraphs'] = num_paragraph\n","        json_data['summary_text'] = summary_to_text(json_data['summary'])\n","        json_data['summary'], num_summary = summary_to_dict_str(json_data['summary'])\n","        json_data['num_of_summary'] = num_summary\n","\n","        new_json_list.append(json_data)\n","\n","    return new_json_list\n","\n","# membuat dataset\n","def create_dataset(jsonl):\n","    header = list(jsonl[0].keys())\n","    dataset_list = []\n","    for json_data in jsonl:\n","        row = []\n","        for h in header:\n","            row.append(json_data[h])\n","        dataset_list.append(row)\n","\n","    return header, dataset_list\n","\n","\n","# membuat dataset dari file\n","def create_dataset_from_files(file_list):\n","    df_header = None\n","    dataset_list = []\n","    for filename in file_list:\n","        json_l = load_file_to_json_list(filename)\n","        new_json_l = alter_json_data(json_l, filename)\n","        header, dataset_part = create_dataset(new_json_l)\n","\n","        if not df_header: df_header = header\n","        dataset_list.extend(dataset_part)\n","\n","    df_full = pd.DataFrame().from_records(dataset_list)\n","    df_full = df_full.rename(columns=dict(enumerate(header)))\n","    return df_full\n","\n"],"metadata":{"id":"d_VgeqimppIU","executionInfo":{"status":"ok","timestamp":1692790227632,"user_tz":-420,"elapsed":11,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# konversi data file menjadi data frame\n","df_test = create_dataset_from_files(test_files)"],"metadata":{"id":"iIhqfR2opprj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790229177,"user_tz":-420,"elapsed":1556,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"fd29cffe-7187-4b75-f5aa-5ccfead781b5"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading data test.01.jsonl: 100%|██████████| 3762/3762 [00:00<00:00, 3838.96it/s]\n","Altering json data test.01.jsonl: 100%|██████████| 3762/3762 [00:00<00:00, 11960.29it/s]\n"]}]},{"cell_type":"code","source":["# konversi dataframe menjadi hugging face dataset\n","data_indosum_test = df_test[['id', 'news_text', 'summary_text']].iloc[0:1000]\n","\n","### konversi data test\n","test_dataset = ds.dataset(pa.Table.from_pandas(data_indosum_test).to_batches())\n","hg_test_dataset = Dataset(pa.Table.from_pandas(data_indosum_test))\n","\n","dataset_bahasa = datasets.DatasetDict({\"test\":hg_test_dataset})"],"metadata":{"id":"UTAHTSaHpuPF","executionInfo":{"status":"ok","timestamp":1692790229178,"user_tz":-420,"elapsed":26,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["dataset_bahasa.column_names"],"metadata":{"id":"xCz_y4c6p-vf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790229178,"user_tz":-420,"elapsed":24,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"fc927628-d0ab-4636-e132-e357d5947176"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'test': ['id', 'news_text', 'summary_text']}"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["dataset_bahasa['test']"],"metadata":{"id":"gCKcGYR5qJYA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790229179,"user_tz":-420,"elapsed":21,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"eee8a866-4a0a-4d69-e8b4-759099c6929a"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'news_text', 'summary_text'],\n","    num_rows: 1000\n","})"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["MELAKUKAN PENGUJIAN PADA 100 ARTIKEL INDOSUM"],"metadata":{"id":"ugB-wrgTqUSy"}},{"cell_type":"code","source":["import evaluate"],"metadata":{"id":"vr5vj88eqMi7","executionInfo":{"status":"ok","timestamp":1692790229180,"user_tz":-420,"elapsed":17,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/f_zero/hasil_uji"],"metadata":{"id":"ALji4np5ftvw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692790229181,"user_tz":-420,"elapsed":17,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"c1d8969b-aabd-4ead-aeea-e97991190d66"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/f_zero/hasil_uji\n"]}]},{"cell_type":"code","source":["def save_test_data(file_name, reference_summary, generated_summary):\n","  # Inisialisasi ROUGE scorer\n","  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","  # Menghitung skor ROUGE untuk setiap pasangan referensi dan hasil yang dihasilkan\n","  scores = []\n","  for ref, gen in zip(reference_summary, generated_summary):\n","      score = scorer.score(ref, gen)\n","      scores.append(score)\n","\n","  # Membuat DataFrame dari skor ROUGE, precision, dan recall\n","  data = {\n","      'Reference': reference_summary,\n","      'Generated': generated_summary,\n","      'ROUGE-1 Precision': [score['rouge1'].precision for score in scores],\n","      'ROUGE-1 Recall': [score['rouge1'].recall for score in scores],\n","      'ROUGE-1 F1-score': [score['rouge1'].fmeasure for score in scores],\n","      'ROUGE-2 Precision': [score['rouge2'].precision for score in scores],\n","      'ROUGE-2 Recall': [score['rouge2'].recall for score in scores],\n","      'ROUGE-2 F1-score': [score['rouge2'].fmeasure for score in scores],\n","      'ROUGE-L Precision': [score['rougeL'].precision for score in scores],\n","      'ROUGE-L Recall': [score['rougeL'].recall for score in scores],\n","      'ROUGE-L F1-score': [score['rougeL'].fmeasure for score in scores],\n","  }\n","\n","  df = pd.DataFrame(data)\n","\n","  # Menyimpan DataFrame ke file Excel\n","  output_excel = file_name\n","  df.to_excel(output_excel, index=False, engine='openpyxl')\n","\n","  return(f\"DataFrame saved to {output_excel}\")\n"],"metadata":{"id":"6oKR_bTYqk0c","executionInfo":{"status":"ok","timestamp":1692790229182,"user_tz":-420,"elapsed":15,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["summary_textrank_bahasa = []\n","summary_pegasus_bahasa = []\n","summary_hybrid_bahasa = []\n","reference_summary_bahasa = []\n","\n","limit = 100"],"metadata":{"id":"bXJMhe-uqoyP","executionInfo":{"status":"ok","timestamp":1692790229182,"user_tz":-420,"elapsed":14,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["for i in range (limit):\n","  # Inisial reference\n","  reference = dataset_bahasa[\"test\"][i][\"summary_text\"]\n","  reference_summary_bahasa.append(reference)"],"metadata":{"id":"3sYOPVjIqxGK","executionInfo":{"status":"ok","timestamp":1692790229183,"user_tz":-420,"elapsed":15,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# textrank rouge score\n","for i in range (limit):\n","  # Inisial kalimat inputan\n","  input = dataset_bahasa[\"test\"][i][\"news_text\"]\n","  result = f_textrank_id(input, 3)\n","\n","  summary_textrank_bahasa.append(result)\n","\n","file_name = 'pengujian_textrank_bahasa.xlsx'\n","\n","print_test = save_test_data(file_name, reference_summary_bahasa, summary_textrank_bahasa)\n","print_test"],"metadata":{"id":"4dIou5BnEULg","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1692790458547,"user_tz":-420,"elapsed":229378,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"efed631c-660f-462a-e337-c361556d9d2c"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'DataFrame saved to pengujian_textrank_bahasa.xlsx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["# pegasus rouge score\n","for i in range (limit):\n","  # Inisial kalimat inputan\n","  input = dataset_bahasa[\"test\"][i][\"news_text\"]\n","  result = f_pegasus_id(input)\n","\n","  summary_pegasus_bahasa.append(result)\n","\n","file_name = 'pengujian_pegasus_bahasa.xlsx'"],"metadata":{"id":"3-Jm2VK0rE9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print_test = save_test_data(file_name, reference_summary_bahasa, summary_pegasus_bahasa)\n","print_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3s-n1LQIzga3","executionInfo":{"status":"ok","timestamp":1692767503702,"user_tz":-420,"elapsed":454,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"79a2366a-5c9f-4a75-c41f-2ab3b1e3913c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'DataFrame saved to pengujian_pegasus_bahasa.xlsx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["# hybrid rouge score\n","for i in range (limit):\n","  # Inisial kalimat inputan\n","  input = dataset_bahasa[\"test\"][i][\"news_text\"]\n","  result = f_hybrid_id(input, 2)\n","\n","  summary_hybrid_bahasa.append(result)\n","\n","file_name = 'pengujian_hybrid_bahasa.xlsx'\n","\n","print_test = save_test_data(file_name, reference_summary_bahasa, summary_hybrid_bahasa)\n","print_test"],"metadata":{"id":"H4OKpi1Nrhpp","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1692771632033,"user_tz":-420,"elapsed":4055224,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"}},"outputId":"3d15adb8-6746-49dd-9421-76bc65daaf5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'DataFrame saved to pengujian_hybrid_bahasa.xlsx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNV/VhCVQXAhUaEZi4TsDVx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}