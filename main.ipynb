{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22737,"status":"ok","timestamp":1694071391090,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"t2zo5gOEo4oT","outputId":"5ac36864-9742-47f9-c49b-6a717c792a3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/681.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m675.8/681.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=462d1d60ec281e6bef8f77f7ccb9cddf87e703174996d786597ed9ac421876f8\n","  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-6.0.0\n","Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n","Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.7)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n"]}],"source":["!pip install pyngrok\n","!pip install flask"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7258,"status":"ok","timestamp":1693198682635,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"MVjyr2WnBHKa","outputId":"c3e1d355-e5f3-40af-ac7d-6c84b6905411"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://download.pytorch.org/whl/cu117\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YX0cE-7rPpup"},"outputs":[],"source":["!pip install transformers[sentencepiece] datasets spacy scipy networkx numpy sent2vec pyngrok pandas nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21939,"status":"ok","timestamp":1693198717026,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"Hb73KE3bPtVk","outputId":"0358d510-3f27-401d-a197-b8e16676279c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# import library yang dibutuhkan\n","import torch\n","import spacy\n","import networkx as nx\n","import numpy as np\n","\n","from scipy import spatial\n","from sent2vec.vectorizer import Vectorizer\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.probability import FreqDist\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"wordnet\")\n","\n","from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","from transformers import TFPegasusForConditionalGeneration, PegasusTokenizerFast\n","from transformers import BertTokenizer, BertModel\n","\n","import re\n","import unicodedata"]},{"cell_type":"markdown","metadata":{"id":"-ki-Rg3NQBVr"},"source":["**PERINGKASAN BAHASA INGGRIS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otwhxqeeQVyY"},"outputs":[],"source":["# 1. FUNGSI PRE-PROCESSING\n","def preprocess_en(text):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  doc = nlp(text)\n","\n","  # Segmentasi (pisahkan text per kalimat dan masukkan kedalam list)\n","  sentences = [sent.text.strip() for sent in doc.sents]\n","\n","  # Membersihkan setiap kalimat\n","  filtered_sentences = []\n","  for i in range(len(sentences)):\n","    doc = nlp(sentences[i])\n","\n","    # inisial variabel untuk menampung kata yang bersih\n","    filtered_tokens = []\n","\n","    # tokenisasi\n","    for token in doc:\n","    # menghilangkan stopword dan karakter yang tidak dibutuhkan\n","      if token.is_stop or token.is_punct:\n","        continue\n","      # lemmatisasi\n","      filtered_tokens.append(token.lemma_)\n","\n","    # how to convert list to string\n","    result = \" \".join(filtered_tokens)\n","\n","    filtered_sentences.append(result)\n","  return filtered_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buR8bDQEZEOE"},"outputs":[],"source":["# Load the tokenizer and model\n","tokenizer_bert_en = BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert_en = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elNAJZ02ZEK5"},"outputs":[],"source":["def vector_en(list_sentences):\n","\n","  # Initialize an empty array to store sentence embeddings\n","  sentence_embeddings = []\n","\n","  # Iterate through sentences and get embeddings\n","  for text in list_sentences:\n","      input_ids = tokenizer_bert_en.encode(text, add_special_tokens=True, padding=True, truncation=True)\n","      input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","      with torch.no_grad():\n","          outputs = model_bert_en(input_ids)\n","          cls_embedding = outputs.last_hidden_state[:, 0, :]\n","          embedding = cls_embedding[0]\n","\n","      sentence_embeddings.append(embedding.numpy())\n","\n","  return sentence_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKlfah9DQcw-"},"outputs":[],"source":["# 2. FUNGSI PERINGKASAN EKSTRAKTIF\n","def extractive_sum_en(filtered_sentences, sentences):\n","  # mengubah kalimat menjadi vektor\n","  vectors = vector_en(filtered_sentences)\n","\n","  # menghitung similarity matrix (matriks kemiripan antar kalimat)\n","  similarity_matrix = []\n","  for i in range(len(vectors)):\n","    row = []\n","    for j in range(len(vectors)):\n","      row.append(spatial.distance.cosine(vectors[i], vectors[j]))\n","    similarity_matrix.append(row)\n","\n","  # konversi matrix menjadi graph\n","  graph = nx.from_numpy_array(np.array(similarity_matrix))\n","\n","  # melakukan perangkingan\n","  scores = nx.pagerank(graph)\n","\n","  # mengambil top rank (kalimat dengan score tertinggi)\n","  sentences_size = len(sentences)\n","  num_sentences = round((sentences_size + 1) / 2)\n","\n","  top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n","  summary = [sentences[i] for i in top_sentence_indices]\n","  summary = \" \".join(summary)\n","\n","  # hasil peringkasan ekstraktif\n","  return summary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52364,"status":"ok","timestamp":1693198775386,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"Xk-gojzpQi53","outputId":"ddb89faf-60a2-4664-a864-14361120d5f7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# 3. FUNGSI PERINGKASAN ABSTRAKTIF\n","\n","model_name1 = \"google/pegasus-cnn_dailymail\"\n","model1 = PegasusForConditionalGeneration.from_pretrained(model_name1)\n","tokenizer1 = PegasusTokenizer.from_pretrained(model_name1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5j9CuuFQ924"},"outputs":[],"source":["def abstractive_sum_en(text):\n","\n","  inputs = tokenizer1.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","\n","  summary_ids = model1.generate(inputs, max_length=250, min_length=50, length_penalty=2.0, num_beams=6, early_stopping=True)\n","\n","  summary = tokenizer1.decode(summary_ids[0], skip_special_tokens=True)\n","\n","  return summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_y28nf3Rkwd"},"outputs":[],"source":["# MERINGKAS TEKS BAHASA INGGRIS\n","def summy_en(text):\n","  document = text\n","  # load spacy\n","  nlp = spacy.load(\"en_core_web_sm\")\n","\n","  # memasukkan kalimat kedalam list (untuk memudahkan print output)\n","  doc = nlp(document)\n","  sentences = [sent.text.strip() for sent in doc.sents]\n","\n","  # melakukan pre-processing\n","  # contoh hasil penggunaan fungsi preprocess\n","  result = preprocess_en(document)\n","\n","  # melakukan peringkasan ekstraktif\n","  summary1 = extractive_sum_en(result, sentences)\n","\n","  # melakukan peringkasan abstraktif\n","  summary = abstractive_sum_en(summary1)\n","  summary = summary.replace(\"<pad>\", \"\")\n","  summary = summary.replace(\"</s>\", \"\")\n","  summary = summary.replace(\".<n>\", \".\\n\")\n","\n","  return summary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECF_qAH4SciT"},"outputs":[],"source":["text = \"\"\"\n","\n","Python is a high-level, interpreted programming language that is widely used for web development, data analysis, artificial intelligence, and many other applications. Created by Guido van Rossum in the late 1980s, Python has become one of the most popular programming languages in the world, thanks to its simplicity, versatility, and wide range of libraries and frameworks.\n","\n","Python is known for its elegant syntax, which emphasizes readability and reduces the cost of program maintenance. Unlike other programming languages that use curly braces and semicolons to denote code blocks and statements, Python uses whitespace and indentation. This makes Python code more concise and easier to read, especially for beginners who are just learning how to program.\n","\n","In addition to its simplicity and readability, Python is also praised for its powerful and flexible features. Python supports object-oriented programming, functional programming, and procedural programming paradigms, making it a versatile tool for different kinds of applications. Python also has a large standard library and a vast ecosystem of third-party libraries and packages, which makes it easy to find and use pre-built modules for different purposes.\n","\n","Whether you are a beginner or an experienced programmer, learning Python can open up many opportunities for you. With its growing popularity and wide range of applications, Python is an essential skill for many industries and fields. So why not start learning Python today and see where it takes you?\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECWQW7NhSkAn"},"outputs":[],"source":["result_en = summy_en(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1693198803156,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"xobXtOgRSsVj","outputId":"24bde366-b7b7-4570-cd7f-1fb609888133"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python supports object-oriented programming, functional programming, and procedural programming paradigms.\n","Unlike other programming languages that use curly braces and semicolons to denote code blocks and statements, Python uses whitespace and indentation.\n"," Python is an essential skill for many industries and fields.\n"]}],"source":["print(result_en)"]},{"cell_type":"markdown","metadata":{"id":"fNSLBwSEQLQ4"},"source":["**PERINGKASAN BAHASA INDONESIA**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hM-FryE9P-Q6"},"outputs":[],"source":["#1. PRE-PROCESSING\n","def preprocess_id(text):\n","\n","  # Segmentasi (pisahkan text per kalimat dan masukkan kedalam list)\n","  sentences = sent_tokenize(text)\n","\n","  # Membersihkan setiap kalimat\n","  filtered_sentences = []\n","  for i in range(len(sentences)):\n","\n","    text = sentences[i]\n","\n","    # Tokenisasi (memecah kalimat menjadi kata)\n","    words = word_tokenize(text.lower())\n","\n","    # Membersihkan tanda baca\n","    words = [word for word in words if word.isalnum()]\n","\n","    # Menghapus stopwords\n","    stop_words = set(stopwords.words(\"indonesian\"))\n","    words = [word for word in words if word not in stop_words]\n","\n","    # Melakukan lemmatisasi\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    # Menggabungkan kembali kata yang telah dibersihkan menjadi kalimat\n","    result = \" \".join(lemmatized_words)\n","\n","    filtered_sentences.append(result)\n","  return filtered_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzkL6ix7ZVxm"},"outputs":[],"source":["# Load the tokenizer and model\n","tokenizer_bert_id = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model_bert_id = BertModel.from_pretrained('bert-base-multilingual-cased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1ePvnRSZdZ7"},"outputs":[],"source":["def vector_id(list_sentences):\n","\n","  # Initialize an empty array to store sentence embeddings\n","  sentence_embeddings = []\n","\n","  # Iterate through sentences and get embeddings\n","  for text in list_sentences:\n","      input_ids = tokenizer_bert_id.encode(text, add_special_tokens=True, padding=True, truncation=True)\n","      input_ids = torch.tensor(input_ids).unsqueeze(0)\n","\n","      with torch.no_grad():\n","          outputs = model_bert_id(input_ids)\n","          cls_embedding = outputs.last_hidden_state[:, 0, :]\n","          embedding = cls_embedding[0]\n","\n","      sentence_embeddings.append(embedding.numpy())\n","\n","  return sentence_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81sWn6YUUH5c"},"outputs":[],"source":["# 2. PERINGKASAN EKSTRAKTIF\n","\n","def extractive_sum_id(filtered_sentences, sentences):\n","  # mengubah kalimat menjadi vektor\n","  vectors = vector_id(filtered_sentences)\n","\n","  # menghitung similarity matrix (matriks kemiripan antar kalimat)\n","  similarity_matrix = []\n","  for i in range(len(vectors)):\n","    row = []\n","    for j in range(len(vectors)):\n","      row.append(spatial.distance.cosine(vectors[i], vectors[j]))\n","    similarity_matrix.append(row)\n","\n","  # konversi matrix menjadi graph\n","  graph = nx.from_numpy_array(np.array(similarity_matrix))\n","\n","  # melakukan perangkingan\n","  scores = nx.pagerank(graph)\n","\n","  # mengambil top rank (kalimat dengan score tertinggi)\n","  sentences_size = len(sentences)\n","  num_sentences = round((sentences_size + 1) / 2)\n","\n","  top_sentence_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentences]\n","  summary = [sentences[i] for i in top_sentence_indices]\n","\n","  # hasil peringkasan ekstraktif\n","\n","  return \" \".join(summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_16U6olJUMnB"},"outputs":[],"source":["# 3. PERINGKASAN ABSTRAKTIF\n","\n","# membersihkan teks\n","def text_cleaning(input_string):\n","    lowercase = input_string.lower()\n","    remove_link = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)', '', lowercase).replace(\"&amp;\",\"&\")\n","    remove_bullet = \"\\n\".join([T for T in remove_link.split('\\n') if '•' not in T and \"baca juga:\" not in T])\n","    remove_accented = unicodedata.normalize('NFKD', remove_bullet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    remove_parentheses = re.sub(\"([\\(\\|]).*?([\\)\\|])\", \"\\g<1>\\g<2>\", remove_accented)\n","    remove_punc = re.sub(r\"[^\\w\\d.\\s]+\",' ', remove_parentheses)\n","    remove_num_dot = re.sub(r\"(?<=\\d)\\.|\\.(?=\\d)|(?<=#)\\.\",\"\", remove_punc)\n","    remove_extra_whitespace =  re.sub(r'^\\s*|\\s\\s*', ' ', remove_num_dot).strip()\n","    return \".\".join([s for s in remove_extra_whitespace.strip().split('.') if len(s.strip())>10]).replace(\"_\",\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11311,"status":"ok","timestamp":1693198822051,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"Pa38ACphUtOv","outputId":"13a83f6d-2bff-4bc8-afa3-3fdd6939a5d7"},"outputs":[{"name":"stderr","output_type":"stream","text":["All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n","\n","All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at thonyyy/pegasus_indonesian_base-finetune.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["model_name2 = \"thonyyy/pegasus_indonesian_base-finetune\"\n","model2 = TFPegasusForConditionalGeneration.from_pretrained(model_name2)\n","tokenizer2 = PegasusTokenizerFast.from_pretrained(model_name2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1q6pf1qNVDXM"},"outputs":[],"source":["def abstractive_sum_id(text):\n","  clean_text = text_cleaning(text)\n","  inputs = tokenizer2(clean_text, return_tensors = 'tf')\n","  summary_ids = model2.generate(**inputs, max_new_tokens = 256, length_penalty=2.0, num_beams=4, early_stopping=True)\n","  summary = tokenizer2.batch_decode(summary_ids, skip_special_tokens=True)\n","\n","  return summary[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbAivhhaVlh3"},"outputs":[],"source":["# MERINGKAS TEKS BAHASA\n","def summy_id(text):\n","\n","  # memasukkan kalimat kedalam list (untuk memudahkan print output)\n","  sentences = sent_tokenize(text)\n","\n","  # 1. PRE-PROCESSING\n","  result = preprocess_id(text)\n","\n","  # 2. PERINGKASAN EKSTRAKTIF\n","  summary1 = extractive_sum_id(result, sentences)\n","\n","  # 3. PERINGKASAN ABSTRAKTIF\n","  summary = abstractive_sum_id(summary1)\n","  summary = summary.replace(\"<pad>\", \"\")\n","  summary = summary.replace(\"</s>\", \"\")\n","  summary = summary.replace(\".<n>\", \".\\n\")\n","\n","  return summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSiDu1RKV6Vq"},"outputs":[],"source":["text = \"\"\"\n","\n","Sampah selalu kita temukan mengotori lingkungan di sekitar kita. Maka wajar karena hal itu seringkali sampah menjadi masalah lingkungan yang serius harus ditangani.\n","\n","Sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. Walaupun sampah jelas-jelas membuat lingkungan tidak nyaman tetapi anehnya kesadaran kita terhadap lingkungan masih jauh dari cukup.\n","\n","Masih banyak di antara kita yang tidak memperhatikan membuang sampah pada tempatnya. Mereka baru menyadari pentingnya membuang sampah secara disiplin, ketika mulai banyak rusaknya lingkungan diakibatkan oleh sampah yang menumpuk.\n","\n","Pada akhirnya kondisi ini telah membuat banyak orang menjadi sadar bahwa mengelola sampah dengan bijak sangatlah penting untuk menjamin rasa nyaman lingkungan juga memperhatikan kesehatan.\n","\n","Gerakan untuk membuang sampah secara disiplin pada tempat sampah semakin ramai digaungkan. Tempat-tempat sampah semakin berlimpah disediakan di depan rumah-rumah. Kondisi ini sungguh sangat menggembirakan.\n","\n","Ibu-ibu pun mulai mengajarkan kepada anak-anak mereka sejak usia dini, betapa pentingnya membuang sampah dengan disiplin pada tempat yang sudah disediakan.\n","\n","Karena hal itu akan menjamin kebersihan lingkungan semakin sehat bagi kehidupan mereka sehari-hari. Disiplin yang ditanamkan sejak usia dini akan menumbuhkan pribadi dengan karakter unggul.\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhZYYMmsWCcD"},"outputs":[],"source":["result_id = summy_id(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1693198844639,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"FMmdy3e0WLL3","outputId":"633d39aa-9805-4aff-b3a4-9819ea17b600"},"outputs":[{"name":"stdout","output_type":"stream","text":["sampah bisa membuat suasana nyaman menjadi rusak seketika karena bau sampah yang menyengat. disiplin yang ditanamkan sejak usia dini akan menumbuhkan pribadi dengan karakter unggul\n"]}],"source":["print(result_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PrlDbmbqK8o"},"outputs":[],"source":["# validasi data inputan\n","def count_word(text):\n","   return len(text.split())"]},{"cell_type":"markdown","metadata":{"id":"T_NWie5DpEHk"},"source":["RUNNING ON FLASK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QcC7191pDsw"},"outputs":[],"source":["from flask import Flask\n","from pyngrok import ngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"674ThmK0pGvk"},"outputs":[],"source":["port_no = 5000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2677,"status":"ok","timestamp":1693198847311,"user":{"displayName":"Raihanun Nisa","userId":"18280284372915918593"},"user_tz":-420},"id":"yEAYnXOhpK1K","outputId":"3e1f4154-8149-49a4-a1a1-3f26fb6e1afa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"W3C-qTwipOSQ","outputId":"08637ae7-7012-47b8-964f-d72134864af8"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:pyngrok.process.ngrok:t=2023-08-28T05:00:46+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"]},{"name":"stdout","output_type":"stream","text":["To access the global link please click https://48cb-35-196-250-47.ngrok-free.app\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:04:59] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:05:00] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:05:05] \"GET /index HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:05:06] \"GET /static/index.css HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:05:06] \"GET /static/people.png HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:06:33] \"POST /index HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:06:34] \"\u001b[36mGET /static/people.png HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:06:34] \"\u001b[36mGET /static/index.css HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:08:11] \"POST /index HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:08:11] \"\u001b[36mGET /static/people.png HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [28/Aug/2023 05:08:11] \"\u001b[36mGET /static/index.css HTTP/1.1\u001b[0m\" 304 -\n"]}],"source":["from flask import url_for, request\n","from flask.templating import render_template\n","template_folder = \"/content/gdrive/MyDrive/NLP/f_zero/templates\"\n","static_folder = \"/content/gdrive/MyDrive/NLP/f_zero/static\"\n","app = Flask(__name__, template_folder=template_folder, static_folder=static_folder)\n","ngrok.set_auth_token(\"2TI0JTA7MxqbjwsmutMmvwUshCV_4b51PZvttuYQYFpur9FJp\")\n","public_url = ngrok.connect(port_no).public_url\n","\n","@app.route(\"/index\", methods=[\"POST\", \"GET\"])\n","def index():\n","  output = \"\"\n","  v_textarea = \"\"\n","  language = \"\"\n","  n_words = 0\n","  if request.method == 'POST':\n","     input_text = request.form['Textarea']\n","     language = request.form['select1']\n","     # validasi jumlah kata minimal 100\n","     n_words = count_word(input_text)\n","     if n_words <= 100:\n","        output = \"*Input sentences are too concise\"\n","     elif n_words > 500:\n","        output = \"*Input more than 500 words\"\n","     else:\n","        if language == \"english\" :\n","          output = summy_en(input_text)\n","        elif language == \"indonesia\" :\n","          output = summy_id(input_text)\n","        else:\n","          output = \"bahasa tidak terdeteksi\"\n","     v_textarea = request.form['Textarea']\n","  return render_template(\"index.html\", output=output, v_textarea = v_textarea)\n","\n","print(f\"To access the global link please click {public_url}\")\n","\n","\n","app.run(port=port_no)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcZYwiSk2WQCcuiQtm3mhi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}